{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functional Prototype Demonstration 2 \n",
    "\n",
    "Team Epsilon-Greedy Quants <br/>\n",
    "Michael Lee, Nikat Patel, Jose Antonio Alatorre Sanchez\n",
    "\n",
    "## What we did in this milestone?\n",
    "\n",
    "We implemented the Policy Gradient algorithms REINFORCE with baseline and Actor-Critic.  We also refactored our model code to work in PyTorch.\n",
    "\n",
    "## Presentation Overview\n",
    "- REINFORCE with Baseline Summary\n",
    "- Actor-Critic Summary\n",
    "- Model Performance Overview\n",
    "- Discussion of Problems Encountered\n",
    "- Code Documentation/Organization\n",
    "- Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## REINFORCE Summary\n",
    "\n",
    "#### Policy Gradient Method \n",
    "- Estimates Policy directly, not from Action-Value function\n",
    "- Continuous action space\n",
    "\n",
    "\n",
    "#### REINFORCE \n",
    "- Performance under Policy-Gradient Theorem: $\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a (q_{\\pi}(s,a)) \\nabla \\pi(a|s, \\theta)$\n",
    "- Relies on estimated return by Monte-Carlo method\n",
    "- Uses episode samples to update policy parameter $\\theta$\n",
    "- **High variance results in slow learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### REINFORCE with Baseline\n",
    "- Compares the action-value to an arbitrary baseline b(s)\n",
    "    - Performance under Policy-Gradient Theorem: $\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a (q_{\\pi}(s,a) - b(s)) \\nabla \\pi(a|s, \\theta)$\n",
    "    - Can be any function or random variable as long as it does not vary with action **a**\n",
    "    - Commonly used baseline: state value function $\\hat{v}(S_t,w)$\n",
    "    - Policy parameter $\\theta$  is updated using baseline: $\\theta_{t+1} = \\theta_t + \\alpha(G_t - b(S_t))\\frac{\\nabla \\pi(A_t| S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)}$\n",
    "- Baseline functions don't change expected value of update but **can reduce the variance** (speed up learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### REINFORCE with Baseline Algorithm Steps\n",
    "From Sutton and Barto, Chapter 13.4\n",
    "\n",
    "**Steps:**\n",
    "- Initialize the policy parameter **$\\theta$** and state-value weights **w** at random.\n",
    "- Loop forever (for each episode):\n",
    "    - Generate one episode using policy $\\pi_{\\theta}: S_0,A_0,R_1,...,S_{T-1},A_{T-1},R_T$.\n",
    "    - Loop for each step of the episode t=0,1,...,T-1:\n",
    "        - Estimate the return $G$ ← $\\sum_{k=t+1}^T \\gamma^{k-t-1} R_k$ \n",
    "        - **Calculate the delta between $G$ and baseline function: $\\delta ← G - \\hat{v}(S_t,w)$**  \n",
    "        - **Update the state-value weights: $w ← w+ \\alpha^w \\delta \\nabla \\hat{v}(S_t, w)$**\n",
    "        - Update policy parameters: $\\theta←\\theta+\\alpha^{\\theta}\\gamma_t \\delta \\nabla ln \\pi(A_t|S_t,\\theta)$\n",
    "            - $\\alpha^{\\theta}$ - stepsize\n",
    "            - $\\gamma$ - discount factor\n",
    "            - $\\nabla ln \\pi(A_t|S_t,\\theta)$ - eligibility vector: gradient of the probability of taking action $A_t$ given a state $S_t$ and policy $\\pi_{\\theta}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actor-Critic Methods\n",
    "\n",
    "In REINFORCE with baseline, the learned state-value function estimates the value of the only the first state of each state transition. This estimate sets a baseline for the subsequent return, but is made prior to the transition’s action and thus cannot be used to assess that action. In actor-critic methods, on the other hand, the state-value function is applied also to the second state of the transition. The estimated value of the second state, when discounted and added to the reward, constitutes the one-step return, $G_{t:t+1}$ which is a useful estimate of the actual return and thus is a way of assessing the action.\n",
    "\n",
    "When the state-value function is used to assess actions in this way it is called a critic, and the overall policy-gradient method is termed an actor-critic method. Note that the bias in the gradient estimate is not due to bootstrapping as such; the actor would be biased even if the critic was learned by a Monte Carlo method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-step Actor-critic\n",
    "\n",
    "One-step actor-critic methods replace the full return of with the one-step return (and use a learned state-value function as the baseline)as follows\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pmb\\theta_{t+1}=\\pmb\\theta_t +\\alpha(G_{t:t+1}-\\hat{\\nu}(S_t,\\pmb{w})\\frac{\\nabla\\pi(A_t|S_t,\\pmb\\theta)}{\\pi(A_t|S_t,\\pmb\\theta)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pmb\\theta_{t+1}=\\pmb\\theta_t +\\alpha(R_{t+1}\\gamma\\hat{\\nu}(S_{t+1},\\pmb{w})-\\hat{\\nu}(S_t,\\pmb{w})\\frac{\\nabla\\pi(A_t|S_t,\\pmb\\theta)}{\\pi(A_t|S_t,\\pmb\\theta)}\n",
    "$$\n",
    "\n",
    "The main appeal of one-step methods is that they are fully online and incremental, yet avoid the complexities of eligibility traces. They are a special case of the eligibility trace methods, but easier to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One step pseudo code:\n",
    "\n",
    "<img src=\"static/img/ac_1_step.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eligibility Traces Actor- Critic\n",
    "\n",
    "The generalizations to the forward view of-step methods and then to a $\\lambda$-return algorithm are straightforward. The one-step return in (1) is merely replaced by $G_{t:t+1}$ or $G{t}^{\\lambda}$respectively. \n",
    "The backward view of the $\\lambda$-return algorithm is also straightforward, using separate eligibility traces for the actor and critic. Pseudocode for the complete algorithm is given in the box below\n",
    "\n",
    "<img src=\"static/img/ac_l_return.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### REINFORCE Results\n",
    "REINFORCE - { Orange, Black }\n",
    "\n",
    "REINFORCE with Baseline - { Green, Blue }\n",
    "\n",
    "<img src=\"reinforce1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Actor Critic Results\n",
    "Actor Critic Without Eligibility Traces - { Orange, Black }\n",
    "\n",
    "Actor Critic With Eligibility Traces - { Green, Blue }\n",
    "\n",
    "<img src=\"ac_6000_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Actor Critic Results\n",
    "Actor Critic Without Eligibility Traces - { Orange, Black }\n",
    "\n",
    "Actor Critic With Eligibility Traces - { Green, Blue }\n",
    "\n",
    "<img src=\"ac_1000_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems Encountered with TensorFlow\n",
    "\n",
    "- Issue: TensorFlow REINFORCE model would not converge\n",
    "    - Used tensorflow.GradientTape() for automatic differentiation\n",
    "    - Experiemented with various keras optimizers\n",
    "\n",
    "\n",
    "- Solution: Refactored Environment to support PyTorch\n",
    "    - PyTorch REINFORCE model converged much faster than our standard REINFORCE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Version Control Repository\n",
    "\n",
    "- We have created a private GitHub repository that contains all our data, documentations, code, and notebooks.\n",
    "- We use version control to develop, update, and collaborate our work.\n",
    "\n",
    "```\n",
    "├── README.md\n",
    "├── data\n",
    "├── data_env\n",
    "│   ├── ief.parquet\n",
    "│   └── spy.parquet\n",
    "├── lib\n",
    "│   ├── Benchmarks.py\n",
    "│   ├── DataHandling.py\n",
    "│   ├── Environment.py\n",
    "│   ├── Environment_refactored.py\n",
    "│   ├── __init__.py\n",
    "├── notebooks\n",
    "│   ├── Benchmark_EDA.ipynb\n",
    "│   ├── ENVIRONMENT.ipynb\n",
    "│   ├── ACTOR_CRITIC.ipynb\n",
    "│   ├── REINFORCE_BASELINE.ipynb\n",
    "│   ├── REINFORCE.ipynb\n",
    "│   ├── data_handler_test.ipynb\n",
    "│   ├── environment_gymai.ipynb\n",
    "│   └── sharpe_sample.ipynb\n",
    "├── static\n",
    "│   └── img\n",
    "│       ├── ac_1_step.png\n",
    "│       └── ac_l_return.png\n",
    "├── temp_persisted_data\n",
    "│   ├── forward_return_dates_simulation_gbm\n",
    "│   ├── only_features_simulation_gbm\n",
    "│   └── only_forward_returns_simulation_gbm\n",
    "├── tests\n",
    "    ├── TestLinearAgent.py\n",
    "    ├── __init__.py\n",
    "    └── test_spin.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next Steps\n",
    "\n",
    "- Experiment with Various Reward Functions to observe differences\n",
    "    - Sortino Ratio to Control Drawdowns. Increase weight of negative rewards if drawdown reaches a certain threshold.\n",
    "\n",
    "- Begin Testing our Models using Real-World DataSets.\n",
    "- Begin Capstone Documentation. \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
