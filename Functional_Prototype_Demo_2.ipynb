{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functional Prototype Demonstration 2 \n",
    "\n",
    "Team Epsilon-Greedy Quants <br/>\n",
    "Michael Lee, Nikat Patel, Jose Antonio Alatorre Sanchez\n",
    "\n",
    "## What we did in this milestone?\n",
    "\n",
    "We implemented the Policy Gradient algorithms REINFORCE with baseline and Actor-Critic.  We also refactored our model code to work in PyTorch.\n",
    "\n",
    "## Presentation Overview\n",
    "- REINFORCE with Baseline Summary\n",
    "- Actor-Critic Summary\n",
    "- Discussion of Problems Encountered\n",
    "- Code Documentation/Organization\n",
    "- Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## REINFORCE with Baseline Summary\n",
    "\n",
    "#### Policy Gradient Method \n",
    "- Estimates Policy directly, not from Action-Value function\n",
    "- Continuous action space\n",
    "\n",
    "\n",
    "#### REINFORCE \n",
    "- Performance under Policy-Gradient Theorem: $\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a (q_{\\pi}(s,a)) \\nabla \\pi(a|s, \\theta)$\n",
    "- Relies on estimated return by Monte-Carlo method\n",
    "- Uses episode samples to update policy parameter $\\theta$\n",
    "- **High variance results in slow learning**\n",
    "\n",
    "#### REINFORCE with Baseline\n",
    "\n",
    "- Compares the action-value to an arbitrary baseline b(s)\n",
    "    - Performance under Policy-Gradient Theorem: $\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a (q_{\\pi}(s,a) - b(s)) \\nabla \\pi(a|s, \\theta)$\n",
    "    - Can be any function or random variable as long as it does not vary with action **a**\n",
    "    - Commonly used baseline: state value function $\\hat{v}(S_t,w)$\n",
    "- Baseline functions don't change expected value of update but **can significantly reduce the variance** (speed up learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### REINFORCE with Baseline Algorithm Steps\n",
    "From Sutton and Barto, Chapter 13.4\n",
    "\n",
    "**Steps:**\n",
    "- Initialize the policy parameter **$\\theta$** and state-value weights **w** at random.\n",
    "- Loop forever (for each episode):\n",
    "    - Generate one episode using policy $\\pi_{\\theta}: S_0,A_0,R_1,...,S_{T-1},A_{T-1},R_T$.\n",
    "    - Loop for each step of the episode t=0,1,...,T-1:\n",
    "        - Estimate the return $G$ ← $\\sum_{k=t+1}^T \\gamma^{k-t-1} R_k$ \n",
    "        - **Calculate the delta between $G$ and baseline function: $\\delta ← G - \\underbrace{\\hat{v}(S_t,w)}_{baseline}$**  \n",
    "        - **Update the state-value weights: $w ← w+ \\alpha^w \\delta \\nabla \\hat{v}(S_t, w)$**\n",
    "        - Update policy parameters: $\\theta←\\theta+\\alpha^{\\theta}\\gamma_t \\delta \\nabla ln \\pi(A_t|S_t,\\theta)$\n",
    "            - $\\alpha^{\\theta}$ - stepsize\n",
    "            - $\\gamma$ - discount factor\n",
    "            - $\\nabla ln \\pi(A_t|S_t,\\theta)$ - eligibility vector: gradient of the probability of taking action $A_t$ given a state $S_t$ and policy $\\pi_{\\theta}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance\n",
    "\n",
    "REINFORCE with Baseline converges 2x faster than REINFORCE given the same contraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance Metrics\n",
    "\n",
    "* In our Benchmarks and Benchmark Utility scripts, we have multiple methods established that we will use in the future to gauge our models performance.\n",
    "\n",
    "```\n",
    "def compute_daily_returns(asset_prices):\n",
    "\t...\n",
    "def compute_covariance_matrix(returns):\n",
    "\t...\n",
    "def compute_expected_portfolio_variance(cov_matrix, weights):\n",
    "\t...\n",
    "def compute_expected_portfolio_volatility(portfolio_variance):\n",
    "\t...\n",
    "def compute_annual_return(returns, weights):\n",
    "\t...\n",
    "def get_normalized_returns(asset_prices):\n",
    "\t...\n",
    "def compute_sharpe_ratio(asset_allocation):\n",
    "\t...\n",
    "def compute_annualized_sharpe_ratio(asset_allocation):\n",
    "\t...\n",
    "def compute_rolling_sharpe_ratio(asset_allocation, window=30):\n",
    "\t...\n",
    "def compute_rolling_returns(asset_allocation, returns_type, window=30):\n",
    "\t...\n",
    "def compute_rolling_volatility(asset_allocation, window=30):\n",
    "\t...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Version Control Repository\n",
    "\n",
    "* We have created a private GitHub repository that contains all our data, documentations, code, and notebooks.\n",
    "* We use version control to develop, update, and collaborate our work.\n",
    "\n",
    "\n",
    "```\n",
    "├── data\n",
    "│   ├── ACWF_ETF__AMEX_1m\n",
    "│   ├── EEMV_ETF__AMEX_1m\n",
    "│   ├── EFAV_ETF__AMEX_1m\n",
    "│   ├── IAU_ETF__AMEX_1m\n",
    "│   ├── IEF_ETF__NASDAQ_1m\n",
    "│   ├── IMTM_ETF__AMEX_1m\n",
    "│   ├── IQLT_ETF__AMEX_1m\n",
    "│   ├── IVLU_ETF__AMEX_1m\n",
    "│   ├── LQD_ETF__AMEX_1m\n",
    "│   ├── MTUM_ETF__AMEX_1m\n",
    "│   ├── QUAL_ETF__AMEX_1m\n",
    "│   ├── USMV_ETF__AMEX_1m\n",
    "│   ├── UUP_ETF__AMEX_1m\n",
    "│   ├── VLUE_ETF__AMEX_1m\n",
    "├── data_env\n",
    "│   ├── ief.parquet\n",
    "│   └── spy.parquet\n",
    "├── lib\n",
    "│   ├── Benchmarks.py\n",
    "│   ├── DataHandling.py\n",
    "│   ├── Environment.py\n",
    "│   └── __init__.py\n",
    "├── notebooks\n",
    "│   ├── Benchmark_EDA.ipynb\n",
    "│   ├── Environment.ipynb\n",
    "│   ├── Environment_Reinforce.ipynb\n",
    "│   ├── REINFORCE.ipynb\n",
    "│   ├── data_handler_test.ipynb\n",
    "│   ├── environment_data_presentation.ipynb\n",
    "│   ├── environment_gymai.ipynb\n",
    "│   └── sharpe_sample.ipynb\n",
    "├── sprints_capstone.docx\n",
    "├── temp_persisted_data\n",
    "│   ├── forward_return_dates_simulation_gbm\n",
    "│   ├── only_features_simulation_gbm\n",
    "│   └── only_forward_returns_simulation_gbm\n",
    "└── tests\n",
    "    ├── TestLinearAgent.py\n",
    "    └── __init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "* Implement Sortino ratio as reward to control drawdown.\n",
    "* Increase weight of negative reward if drawdown reaches a certain level.\n",
    "* Test our models on real-world data\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "e599a_py37",
   "language": "python",
   "name": "e599a_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
