{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \n",
    "    def __init__(self,features,number_of_assets,bars_count,commission,close_returns):\n",
    "        \n",
    "        \n",
    "        assert features.index.equals(close_returns.index)\n",
    "        \n",
    "        self.features=features\n",
    "        self.number_of_assets=number_of_assets\n",
    "        self.bars_count=bars_count\n",
    "        self.percent_commission=commission\n",
    "        self.close_returns=close_returns\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the weights_buffer\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self._initialize_weights_buffer()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _initialize_weights_buffer(self):\n",
    "        #TODO: Should this be part of state or environment?\n",
    "        \"\"\"\n",
    "         :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        self.weight_buffer=self.features*0+1/self.number_of_assets\n",
    "        \n",
    "    @property\n",
    "    def shape(self):\n",
    "        raise\n",
    "    def _set_weights_on_date(weights,target_date):\n",
    "        self.weight_buffer.loc[target_date]=weights\n",
    "        \n",
    "    def step(self, action, action_date):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param action: corresponds to portfolio weights np.array(n_assets,1)\n",
    "        :param action_date: datetime.datetime\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        #get previous allocation\n",
    "        \n",
    "        \n",
    "        \n",
    "        action_date_index=np.argmax(self.weight_buffer.index.isin([action_date]))\n",
    "        self._set_weights_on_date(weights=action,target_date=action_date)\n",
    "        \n",
    "        \n",
    "        weight_difference=self.weight_buffer.iloc[action_date_index-1:action_date_index+1]\n",
    "        #obtain the difference from the previous allocation, diff is done t_1 - t\n",
    "        weight_difference=abs(weight_difference.diff().dropna())\n",
    "        \n",
    "        #calculate rebalance commission\n",
    "        commision_percent_cost=-weight_difference.sum(axis-1)*self.percent_commission\n",
    "        \n",
    "        #get period_ahead_returns\n",
    "        t_plus_one_returns=self.close_returns.iloc[action_date_index]\n",
    "        one_period_mtm_reward=(t_plus_one_returns*action).sum()\n",
    "        \n",
    "        reward=one_period_mtm_reward-commision_percent_cost\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def encode(self,date):\n",
    "        \"\"\"\n",
    "        convert current state to tensor\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    meta_parameters:\n",
    "            bars_count=the number of bars that we pass on each observation\n",
    "            commission\n",
    "            reward_funtion:\n",
    "                -cummulative_return_over_batch\n",
    "                -defined_forecast_frequency\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "meta_parameters={\"bars_count\":30}\n",
    "\n",
    "class DeepTradingEnvironment(gym.Env):\n",
    "    metadata={'render.modes':['human']}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dirs_and_transform(cls,meta_parameters,data_dir=\"data_env\",**kwargs):\n",
    "        \"\"\"\n",
    "        Do transformations that shouldnt be part of the class\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #optimally this should be only features\n",
    "        features={file:pd.read_parquet(data_dir+\"/\"+file)[\"close\"] for file in os.listdir(data_dir)}\n",
    "        \n",
    "        \n",
    "        features=pd.DataFrame(features)\n",
    "        \n",
    "        assets_prices=features\n",
    "        #transform features\n",
    "        \n",
    "        return DeepTradingEnvironment(features,\n",
    "                                      assets_prices,\n",
    "                                      meta_parameters,**kwargs)\n",
    "        \n",
    "    \n",
    "    def __init__(self,features,assets_prices,meta_parameters,state_class_name=\"State\"):\n",
    "        \"\"\"\n",
    "        features: pandas.DataFrame with features by time\n",
    "        asset_prices=pandas.DataFrame with asset prices by time\n",
    "        \"\"\"\n",
    "        \n",
    "        assert features.index.equals(assets_prices.index)\n",
    "        \n",
    "        self.features=features\n",
    "        self.number_of_features=len(self.features.columns)\n",
    "        self.assets_prices=assets_prices\n",
    "        self.number_of_assets=len(self.assets_prices.columns)\n",
    "        self.close_returns=np.log(self.assets_prices).diff()\n",
    "        \n",
    "        self.meta_parameters=meta_parameters\n",
    "        \n",
    "        \n",
    "        #logic to create state\n",
    "        self._state=State(features=features,number_of_assets=self.number_of_assets,\n",
    "                          bars_count=self.meta_parameters[\"bars_count\"],\n",
    "                         commission=self.meta_parameters[\"bars_count\"],\n",
    "                         close_returns=self.close_returns)\n",
    "        \n",
    "        \n",
    "        # crete action and observation space members\n",
    "        \n",
    "        #action space is the portfolio weights at any time in our example it is bounded by [0,1]\n",
    "        self.action_space=gym.spaces.Box(low=0,high=1,shape=(self.number_of_assets,))\n",
    "        \n",
    "        \n",
    "        #features to be scaled normal scaler will bound them in -4,4\n",
    "        self.observation_space=gym.spaces.Box(low=-4, high=4,shape=(self.number_of_features,))\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        resets the environment:\n",
    "            -resets the buffer of weights in the environments\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    def step(self, action_portfolio_weights, action_date):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param action_portfolio_weights: \n",
    "        :param action_date: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        action=action_portfolio_weights\n",
    "        reward,done=self._state.step(action,action_date)\n",
    "        obs=self._state.encode()\n",
    "        info={\"action\":None,\n",
    "             \"date\":action_date}\n",
    "        \n",
    "        return obs,reward,done, info\n",
    "    \n",
    "        \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "        \n",
    "env=DeepTradingEnvironment.from_dirs_and_transform(meta_parameters=meta_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One dimension continuos action\n",
    "\n",
    "$$\n",
    "\\pi(a|s,\\theta)=\\frac{1}{\\sigma(s,\\theta)\\sqrt{2\\pi}}exp(-\\frac{(a-\\mu(s,\\theta))^2}{2\\sigma(s,\\theta)})\n",
    "$$\n",
    "\n",
    "We parametrize mu as we pleased. Simples parametrization\n",
    "\n",
    "$$\n",
    "\\mu(s,\\theta)=\\theta_{\\mu}^Tx(s)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma(s,\\theta)=exp(\\theta_{\\sigma}^Tx(s))\n",
    "$$\n",
    "\n",
    "Now \n",
    "\n",
    "$$\n",
    "\\theta=[\\theta_{\\mu},\\theta_{\\sigma}]\n",
    "$$\n",
    "\n",
    "\n",
    "Initialize variance to be large to have explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Algorithm  REINFORCE\n",
    "\n",
    "\n",
    "#### State\n",
    "\n",
    "State contains historical features on the time window as well as the latest weights. \n",
    "\n",
    "#### Meta parameters\n",
    "\n",
    "* in_window:  number of previous observations that each state will include for example if data is in intervals of 10 minutes and in_window=20 then will be 200 minutes time frame.\n",
    "* forecast_window: time frame for the action to wait for the reward\n",
    "* episode_length: the number of samples of  states for each episode. On each update it doesnt make sense to run continous time windows. Specially because our states will be heavily correlated.  We can also see this as the size of the batch if we use a neural network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Pseudo-Code:\n",
    "\n",
    "Loop forever ( for each episode):\n",
    "\n",
    "    (episode_index_sample)Take a random sample of size episode_length on all the dates in the training sample\n",
    "    {We can test with non continuos and continuos batches}\n",
    "    Generate an episode S0,A0,R1 ..... (To generate A0 use policy sampling\n",
    "    Save actions to weights buffer\n",
    "    loop[ for each step of episode t=0,,T-1]\n",
    "        \n",
    "        calculate reward if not continous then G shouldnt be discounted\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "**Example**\n",
    "in_window=10\n",
    "forecast_window=10 minutes\n",
    "episode_lenght=200\n",
    "\n",
    "```\n",
    "episode_index_sample=[11,45,...,100]\n",
    "S0=data[11-10:11]\n",
    "A0=[,1,.2...,7]\n",
    "R=log_return from data[11:+10_minutes]\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p37_venv_class",
   "language": "python",
   "name": "p37_venv_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
