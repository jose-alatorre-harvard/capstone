{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Data Demonstration\n",
    "\n",
    "### Team Epsilon-Greedy Quants\n",
    "#### Michael Lee, Nikat Patel, Jose Antonio Alatorre Sanchez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates what a user sees when the user runs the User Guide for Real-World Data Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.e_greedy import DeepTradingEnvironment, LinearAgent\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt.plotting import plot_efficient_frontier\n",
    "from pypfopt.cla import CLA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import copy\n",
    "import quantstats as qs\n",
    "qs.extend_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "data_env = root+\"/data_env/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_asset_dict():\n",
    "    # obtain close prices from parquet files of ETF price history\n",
    "    root = os.getcwd()\n",
    "    data_env = root+\"/data_env/\"\n",
    "    files = [_ for _ in os.listdir(data_env) if \"parquet\" in _]\n",
    "    assets_dict = {file: pd.read_parquet(data_env + \"/\" + file) for file in files}\n",
    "    counter=0\n",
    "    for key, value in assets_dict.items():\n",
    "        if counter==0:\n",
    "            main_index=value.index\n",
    "    else:\n",
    "        main_index=main_index.join(value.index,how=\"inner\")\n",
    "        \n",
    "    for key, value in assets_dict.items():\n",
    "        tmp_df=value.reindex(main_index)\n",
    "        tmp_df=tmp_df.fillna(method='ffill')\n",
    "        assets_dict[key]=tmp_df['close']\n",
    "    return assets_dict\n",
    "\n",
    "def build_portfolio_df(asset_dict):\n",
    "    portfolio_df = pd.DataFrame()\n",
    "    \n",
    "    for key, value in assets_dict.items():\n",
    "        key = key.split(\".\")[0]\n",
    "        tmp_df = pd.DataFrame(data=value)\n",
    "        tmp_df.columns=[key]\n",
    "        portfolio_df = pd.concat([portfolio_df, tmp_df], axis=1)\n",
    "        \n",
    "    portfolio_df.index = pd.to_datetime(portfolio_df.index, errors='coerce')\n",
    "    return portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_backtest(linear_agent_train, env_test, test_input, model):\n",
    "    ## Create plot of backtest returns\n",
    "    if not \"backtest\" in locals():\n",
    "        backtest=None\n",
    "    backtest, tmp_weights =linear_agent_train.backtest_policy(epoch=1,backtest=backtest, env_test=env_test, test_input=test_input)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(backtest,color=\"blue\")\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xlabel(\"Date\", fontsize = 10)\n",
    "    plt.ylabel(\"Backtest\", fontsize = 10)\n",
    "    plt.title(\"Backtest on Test Data: \"+ model,fontsize = 16)\n",
    "    plt.savefig(root+'/temp_persisted_data/backtest_'+model+'.png')\n",
    "    tmp_weights.to_csv(root+'/temp_persisted_data/backtest_weights_'+model+'.csv')\n",
    "    plt.show()\n",
    "    return backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Real-World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a sample ETF\n",
    "pd.read_parquet(data_env+'EEMV.parquet').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a portfolio\n",
    "assets_dict = _retrieve_asset_dict()\n",
    "portfolio_df = build_portfolio_df(assets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train dataset and de-mean the time series\n",
    "\n",
    "portfolio_df_train = portfolio_df[portfolio_df.index <= '2020-04-01']\n",
    "portfolio_df_train.sub(portfolio_df_train.mean())\n",
    "\n",
    "portfolio_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test dataset consisting of 6 months of data and de-mean the time series\n",
    "\n",
    "portfolio_df_test = portfolio_df[portfolio_df.index >= '2020-04-16']\n",
    "portfolio_df_test = portfolio_df_test[portfolio_df_test.index <= '2020-11-16']\n",
    "portfolio_df_test.sub(portfolio_df_test.mean())\n",
    "\n",
    "portfolio_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_returns = portfolio_df_test.to_returns().dropna()\n",
    "test_input_returns = test_input_returns.loc[(test_input_returns != 0).any(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters related to the transformation of data, this parameters govern an step before the algorithm\n",
    "out_reward_window=datetime.timedelta(days=7)\n",
    "\n",
    "meta_parameters = {\"in_bars_count\": 14,\n",
    "                   \"out_reward_window\":out_reward_window ,\n",
    "                   \"state_type\":\"in_window_out_window\",\n",
    "                   \"risk_aversion\":10,\n",
    "                   \"include_previous_weights\":False}\n",
    "\n",
    "# parameters that are related to the objective/reward function construction\n",
    "objective_parameters = {\"percent_commission\": .001}\n",
    "\n",
    "print(\"===Meta Parameters===\")\n",
    "print(meta_parameters)\n",
    "print(\"===Objective Parameters===\")\n",
    "print(objective_parameters)\n",
    "\n",
    "# create an environment and build features based on Real-World Dataset located in the \"data_env\" folder \n",
    "env = DeepTradingEnvironment.build_environment_from_dirs_and_transform(meta_parameters, objective_parameters,data_hash=\"real_data\", data_dir=\"data_env\")\n",
    "\n",
    "number_of_assets = env.number_of_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Features and Forward Returns into Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_parquet(\"temp_persisted_data/only_features_real_data\")\n",
    "\n",
    "features_train = features[features.index <= '2020-04-01']\n",
    "features_train.sub(features_train.mean()) \n",
    "\n",
    "features_test = features[features.index >= '2020-04-16']\n",
    "features_test = features_test[features_test.index <= '2020-11-16']\n",
    "features_test.sub(features_test.mean())\n",
    "\n",
    "features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_return_dates = pd.read_parquet(\"temp_persisted_data/forward_return_dates_real_data\")\n",
    "\n",
    "forward_return_dates_train = forward_return_dates[forward_return_dates.index <= '2020-04-01']\n",
    "\n",
    "forward_return_dates_test = forward_return_dates[forward_return_dates.index > '2020-04-16']\n",
    "forward_return_dates_test = forward_return_dates_test[forward_return_dates_test.index <= '2020-11-16']\n",
    "\n",
    "forward_return_dates_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_returns = pd.read_parquet(\"temp_persisted_data/only_forward_returns_real_data\")\n",
    "\n",
    "forward_returns_train = forward_returns[forward_returns.index <= '2020-04-01']\n",
    "forward_returns_train.sub(forward_returns_train.mean()) \n",
    "\n",
    "forward_returns_test = forward_returns[forward_returns.index >= '2020-04-16']\n",
    "forward_returns_test = forward_returns_test[forward_returns_test.index <= '2020-11-16']\n",
    "forward_returns_test.sub(forward_returns_test.mean()) \n",
    "\n",
    "forward_returns_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Policy-Gradient Method Algorithms on Real-World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 4000\n",
    "model_run = \"demeaned_return_reward_variance_risk_10_\"\n",
    "sample_observations = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and run REINFORCE\n",
    "\n",
    "env_reinforce_train=DeepTradingEnvironment(features_train, forward_returns_train, forward_return_dates_train, objective_parameters,\n",
    "                 meta_parameters)\n",
    "env_reinforce_test = DeepTradingEnvironment(features_test, forward_returns_test, forward_return_dates_test, objective_parameters,\n",
    "                 meta_parameters)\n",
    "\n",
    "linear_agent_reinforce = LinearAgent(environment=env_reinforce_train,out_reward_window_td=out_reward_window, reward_function=\"return_with_variance_risk\",sample_observations=sample_observations)\n",
    "linear_agent_reinforce.REINFORCE_fit(max_iterations=max_iter, add_baseline=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backtest \n",
    "backtest_reinforce = plot_backtest(linear_agent_reinforce, env_reinforce_test, portfolio_df_test, model=\"REINFORCE\")\n",
    "backtest_reinforce.to_csv('temp_persisted_data/'+model_run+'backtest_reinforce.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_input_returns+1).cumprod().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_input_returns.sum(axis=1)+1).cumprod().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and run REINFORCE with baseline\n",
    "env_reinforce_baseline_train = DeepTradingEnvironment(features_train, forward_returns_train, forward_return_dates_train, objective_parameters,\n",
    "                 meta_parameters)\n",
    "env_reinforce_baseline_test = DeepTradingEnvironment(features_test, forward_returns_test, forward_return_dates_test, objective_parameters,\n",
    "                 meta_parameters)\n",
    "\n",
    "linear_agent_reinforce_baseline = LinearAgent(environment=env_reinforce_baseline_train,out_reward_window_td=out_reward_window, reward_function=\"return_with_variance_risk\",sample_observations=sample_observations)\n",
    "linear_agent_reinforce_baseline.REINFORCE_fit(max_iterations=max_iter, add_baseline=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backtest \n",
    "backtest_reinforce_baseline = plot_backtest(linear_agent_reinforce_baseline, env_reinforce_baseline_test, portfolio_df_test, model=\"REINFORCE with Baseline\")\n",
    "backtest_reinforce_baseline.to_csv('temp_persisted_data/'+model_run+'backtest_reinforce_baseline.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and run Actor-Critic \n",
    "\n",
    "env_actor_critic_no_trace_train = DeepTradingEnvironment(features_train, forward_returns_train, forward_return_dates_train, objective_parameters,\n",
    "                 meta_parameters)\n",
    "env_actor_critic_no_trace_test = DeepTradingEnvironment(features_test, forward_returns_test, forward_return_dates_test, objective_parameters,\n",
    "                 meta_parameters)\n",
    "\n",
    "linear_agent_actor_critic_no_trace = LinearAgent(environment=env_actor_critic_no_trace_train,out_reward_window_td=out_reward_window, reward_function=\"return_with_variance_risk\",sample_observations=sample_observations)\n",
    "linear_agent_actor_critic_no_trace.ACTOR_CRITIC_FIT(use_traces=False,max_iterations=max_iter, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backtest\n",
    "backtest_actor_critic_no_trace = plot_backtest(linear_agent_actor_critic_no_trace, env_actor_critic_no_trace_test,  portfolio_df_test, model=\"Actor-Critic without Eligibility Traces\")\n",
    "backtest_actor_critic_no_trace.to_csv('temp_persisted_data/'+model_run+'backtest_actor_critic_no_trace.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic with Eligibility Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment and run Actor-Critic with Eligibility Traces \n",
    "env_actor_critic_trace_train = DeepTradingEnvironment(features_train, forward_returns_train, forward_return_dates_train, objective_parameters,\n",
    "                 meta_parameters)\n",
    "env_actor_critic_trace_test = DeepTradingEnvironment(features_test, forward_returns_test, forward_return_dates_test, objective_parameters,\n",
    "                 meta_parameters)\n",
    "\n",
    "linear_agent_actor_critic_trace = LinearAgent(environment=env_actor_critic_trace_train,out_reward_window_td=out_reward_window, reward_function=\"return_with_variance_risk\",sample_observations=sample_observations)\n",
    "linear_agent_actor_critic_trace.ACTOR_CRITIC_FIT(use_traces=True,max_iterations=max_iter, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backtest \n",
    "backtest_actor_critic_trace = plot_backtest(linear_agent_actor_critic_trace, env_actor_critic_trace_test,  portfolio_df_test, model=\"Actor-Critic with Eligibility Traces\")\n",
    "backtest_actor_critic_trace.to_csv('temp_persisted_data/'+model_run+'backtest_actor_critic_trace.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e599a_py37",
   "language": "python",
   "name": "e599a_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
