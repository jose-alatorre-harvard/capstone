{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Demonstration\n",
    "\n",
    "\n",
    "In this capstone we explored the usage of Reinforcement Learningfor portfolio construction and enhancement of quantitative investmentstrategies (QIS). Particularly we explored the usage of Policy GradientMethods (PGM) due to their ability to handle continuous action spaces.We used PGM to create a model free agent that selects portfolio weights according to a diverse set of features that we considered as space. The PGM explored are: REINFORCE, REINFORCEwith baseline, Actor-Critic, Actor-Critic with eligibility traces and soft-actor Critic.\n",
    "\n",
    "Exploring model-free reinforcement leanring algorithms in portfolio allocation that can be generalized to any type of features provides ground work to integrate signal discovery into portfolio allocation. \n",
    "\n",
    "\n",
    "## Stones on the way ( Problems we faced)\n",
    "\n",
    "We will focus particularly on the problem that we faced related to the proposed models.\n",
    "\n",
    "1. Slow convergence : REINFORCE and REINFORCE with baselines experienced extremely slow convergence in the test set. This make us consider the impracticity of the algorithms for real world solutions where the number of features are complexity of the data require a higher dimensional space. \n",
    "\n",
    "2. Complex model pipelines: RL implementation requires more complex model pipelines than other machine learning models due to the necesity of creating different assets likes:environment, actors and policies. The interaction of assets in the algorithm creates a complex relation that is not simple to paralelize or transport to other devices. For example; in Soft Actor Critic, the agent  has a 4 architectures one model for the policy mean one for the policy variance and 2 for a twin Q function. Each of this model is a Neural Network that needs to be trained in synchrony at each step. \n",
    "\n",
    "3. Sampling efficiency. As with any reinforcement leanring algorithm, a great amount of time is spent in sampling sars from the environment.\n",
    "\n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test and proper model function\n",
    "\n",
    "For the control dataset, we simulated different assets using a classical geometric Brownian motion process for each of the assets i.e.\n",
    "\n",
    "$$\n",
    "dS_t=\\mu S_tdt+\\sqrt{\\sigma}S_tdB_t\n",
    "$$\n",
    "\n",
    "The control data set is built to measure the porformance of each model/algorithm against known solutions given a constant drift and a constant volatility. \n",
    "\n",
    "We measured each algorithm on a 2-asset simulated data using two different reward windows.\n",
    "\n",
    "1. Next period return: On each observation the agent gets as reward the return of the portfolio for the next period. \n",
    "2. Negative of squared return : On each observation the agent gets as reward the negative of the squared return of the portfolio in the next period. \n",
    "\n",
    "With only two assets we expect that our algorithm will converge to the asset with highest return for the next period return reward and to the asset with the smaller volatility in the second reward. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p37_deep_trading",
   "language": "python",
   "name": "p37_deep_trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
